{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Fought the Law:\n",
    "### Contesting Charges in Virginia's District Courts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"internet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "Data for this project was pulled from Ben Schoenfeld's repository of scraped Virginia Court Cases. I used district criminal court information for 2017, which amounted to 2.1m rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "To answer the problem statement, I needed to engineer several features:\n",
    "1. Positive Outcome for the Defense: final dispositions that were 'Not Guilty', 'Dismissed', 'Nolle Prosequi', 'Not Guilty Due to Insanity', and 'Amended Charge'.\n",
    "2. Type of Defense: differentiate hearings by defendants who had a Private Lawyer, Public Defender, or no hired defense (defended themselves)\n",
    "3. Types of Charges: the original dataset had 66,000+ unique charge descriptions and over 5,000 charge codes associated with them. To quickly bucket these into more manageable categories, I used topic modeling to cluster charges into 59 charge categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as pytools\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.offline as pyo\n",
    "import cufflinks as cf\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pytools.set_credentials_file(username='katerdowdy', api_key='hBCWsR3iY9a1feRSpU2A')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.read_csv('./summary_county_data.csv')\n",
    "df_agg = pd.read_csv('./aggregate_charge_data.csv')\n",
    "df_full = pd.read_csv('./2017_full.csv')\n",
    "\n",
    "df_summary = df_summary.sort_values(by = 'Court')\n",
    "counties = list(df_summary['Court'].unique())\n",
    "df_agg = df_agg.sort_values(by = 'ChargeType')\n",
    "charges = list(df_agg['ChargeType'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chloropleth(selection):\n",
    "    try:\n",
    "        fips = df_summary['full_fips']\n",
    "        values = df_summary[selection]\n",
    "        step1 = values.mean() / 5\n",
    "        step2 = (max(values) - values.mean()) / 5\n",
    "        step3 = (max(values) / 10)\n",
    "        num_endpoints = [np.round((values.mean() - (step1 * 4)), -3),\n",
    "                    np.round((values.mean() - (step1 * 3)), -3),\n",
    "                    np.round((values.mean() - (step1 * 2)), -3),\n",
    "                    np.round((values.mean() - step1), -3),\n",
    "                    np.round((values.mean()), -3),\n",
    "                    np.round((max(values) - (step2 * 4)), -3),\n",
    "                    np.round((max(values) - (step2 * 3)), -3),\n",
    "                    np.round((max(values) - (step2 * 2)), -3),\n",
    "                    np.round((max(values) - step2), -3)]\n",
    "        prop_endpoints = [.1, .2, .3, .4, .5, .6, .8, 1, 1.2]\n",
    "        even_endpoints = [step3, (step3 * 2), (step3 * 3), (step3 * 4), (step3 * 5),\n",
    "                         (step3 * 6), (step3 * 7), (step3 * 8), (step3 * 9)]\n",
    "    \n",
    "        colorscale = [\"#eafcfd\", \"#b7e0e4\", \"#85c5d3\", \"#60a7c7\", \"#4989bc\",\n",
    "               \"#3e6ab0\", \"#3d4b94\", \"#323268\", \"#1d1d3b\", \"#030512\"]\n",
    "        \n",
    "        fig = ff.create_choropleth(fips = fips, \n",
    "                           values = values,\n",
    "                          scope = ['VA'],\n",
    "                          county_outline={'color': 'rgb(169,169,169)', 'width': 1},\n",
    "                           exponent_format=True,\n",
    "                           #binning_endpoints = hearing_endpoints,\n",
    "                           binning_endpoints = num_endpoints,\n",
    "                          colorscale = colorscale,\n",
    "                           legend_title=selection)\n",
    "        return py.iplot(fig, filename=selection)\n",
    "            \n",
    "    except:\n",
    "        fig = ff.create_choropleth(fips = fips, \n",
    "                           values = values,\n",
    "                          scope = ['VA'],\n",
    "                          county_outline={'color': 'rgb(169,169,169)', 'width': 1},\n",
    "                           exponent_format=True,\n",
    "                           #binning_endpoints = hearing_endpoints,\n",
    "                           binning_endpoints = even_endpoints,\n",
    "                          colorscale = colorscale,\n",
    "                           legend_title=selection)\n",
    "        return py.iplot(fig, filename=selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['county_hearings', 'county_fines_charged', 'county_sentencing',\n",
    "          'county_probation', 'defense_win_rate', 'Population',\n",
    "          'fines_per_capita', 'hearings_per_capita']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends Across Virginia\n",
    "#### Hearings by County\n",
    "The number of hearings (corresponding to the number of tickets/arrests) seem to be higher along the I-95 and I-81 corridors and in high-density population areas (Northern Virginia, Richmond, and Virginia Beach/Norfolk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~katerdowdy/11.embed\" height=\"450px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chloropleth('county_hearings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where Defenses Are Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~katerdowdy/19.embed\" height=\"450px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chloropleth('defense_win_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chloro = interactive(chloropleth, selection = ['county_hearings', 'county_fines_charged', \n",
    "                                               'county_sentencing',\n",
    "          'county_probation', 'defense_win_rate', 'Population',\n",
    "          'fines_per_capita', 'hearings_per_capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agg_charges(casetype):\n",
    "    trace1 = go.Bar(\n",
    "        x=df_agg[df_agg['CaseType'] == casetype]['ChargeType'],\n",
    "        y=df_agg[df_agg['CaseType'] == casetype]['agg_charge'],\n",
    "        name='All Charges'\n",
    "    )\n",
    "    trace2 = go.Bar(\n",
    "        x=df_agg[df_agg['CaseType'] == casetype]['ChargeType'],\n",
    "        y=df_agg[df_agg['CaseType'] == casetype]['agg_charge'] * df_agg[df_agg['CaseType'] == casetype]['agg_contested_rate'],\n",
    "        name='Defendants Went to Court'\n",
    "    )\n",
    "\n",
    "    trace3 = go.Bar(\n",
    "        x=df_agg[df_agg['CaseType'] == casetype]['ChargeType'],\n",
    "        y=df_agg[df_agg['CaseType'] == casetype]['agg_charge'] * df_agg[df_agg['CaseType'] == casetype]['agg_charge_overturn_rate'],\n",
    "        name='Charges Dismissed/Overturned/Amended'\n",
    "    )\n",
    "\n",
    "    data = [trace1, trace2, trace3]\n",
    "    \n",
    "    layout = go.Layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=800,\n",
    "    barmode='group',\n",
    "    xaxis=dict(\n",
    "        title='CHARGES',\n",
    "        titlefont=dict(\n",
    "            family='Arial, sans-serif',\n",
    "            size=18,\n",
    "            color='lightgrey'\n",
    "        ),\n",
    "        showticklabels=True,\n",
    "        automargin=True,\n",
    "        tickangle=45,\n",
    "        tickfont=dict(\n",
    "            family='Arial, sans-serif',\n",
    "            size=14,\n",
    "            color='black'\n",
    "        ),\n",
    "        exponentformat='e',\n",
    "        showexponent='all'\n",
    "    ),\n",
    ")\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return py.iplot(fig, filename='grouped-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Charges are Contested / Defeated Most?\n",
    "### Infractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~katerdowdy/8.embed\" height=\"800px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_agg_charges(casetype = 'Infraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misdemeanors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~katerdowdy/8.embed\" height=\"800px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_agg_charges(casetype = 'Misdemeanor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Felonies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~katerdowdy/8.embed\" height=\"800px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_agg_charges(casetype = 'Felony')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Civil Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~katerdowdy/8.embed\" height=\"800px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_agg_charges(casetype = 'Civil Violation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Well Can We Predict the Outcome of a Case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features # complainant\n",
    "drop_features = ['Unnamed: 0',\n",
    "                 'level_0',\n",
    "                             'index', \n",
    "                             'HearingDate', \n",
    "                             'HearingResult', \n",
    "                             #'HearingPlea',\n",
    "                             'HearingContinuanceCode',\n",
    "                             'HearingType',\n",
    "                             'HearingCourtroom',\n",
    "                             'fips',\n",
    "                             'FiledDate',\n",
    "                             'Locality',\n",
    "                             'Status',  \n",
    "                             'Address',\n",
    "                             'Gender',\n",
    "                             'Race',\n",
    "                             'Charge', \n",
    "                             'CodeSection', \n",
    "                             'Contested',\n",
    "                             'CaseType', \n",
    "                             'Class',\n",
    "                             'OffenseDate', \n",
    "                             'ArrestDate', \n",
    "                             'AmendedCharge',\n",
    "                             'AmendedCode', \n",
    "                             'AmendedCaseType', \n",
    "                             'FinalDisposition',\n",
    "                             'ProbationTime', \n",
    "                             'ProbationStarts',\n",
    "                             'SentenceTime', \n",
    "                             'SentenceSuspendedTime', \n",
    "                             'ProbationType',\n",
    "                             'OperatorLicenseSuspensionTime',\n",
    "                               'RestrictionEffectiveDate', \n",
    "                             'RestrictionEndDate',\n",
    "                               'OperatorLicenseRestrictionCodes', \n",
    "                             'Fine', \n",
    "                             'Costs', \n",
    "                             'FineCostsDue',\n",
    "                               'FineCostsPaid', \n",
    "                             'FineCostsPaidDate', \n",
    "                             'VASAP', \n",
    "                             'FineCostsPastDue',\n",
    "                             'person_id', \n",
    "                             'person_id_freq',\n",
    "                             'full_fips',\n",
    "                             'Outcome_Positive', \n",
    "                             'Amended', \n",
    "                             'Total_Positive',\n",
    "                            'ChargeType',\n",
    "                            'Court']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['DefenseAttorney'].fillna(0, inplace = True)\n",
    "df_full['Complainant'].fillna(0, inplace = True)\n",
    "df_full['HearingPlea'].fillna(0, inplace = True)\n",
    "\n",
    "def log_odds(x):\n",
    "    return np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "df = df_full[(df_full['Court'] == 'Fairfax County') & (df_full['ChargeType'] == 'MIS: Assault')]\n",
    "df_dummied = pd.get_dummies(df, columns = ['DefenseAttorney', 'Complainant', 'HearingPlea'], drop_first = True)\n",
    "df_model = df_dummied[df_dummied['Contested'] == 1]\n",
    "X = df_model.drop(columns = drop_features)\n",
    "features = X.columns\n",
    "y = df_model['Total_Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real_test = pd.DataFrame(index = [0], columns = [features])\n",
    "X_real_test.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real_test.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "had_lawyer_options = ['Yes', 'No']\n",
    "public_defender = ['Yes', 'No']\n",
    "prior_hearings = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "plea = ['Guilty', 'Not Guilty', 'Nolo Contendere', 'Tried In Absentia', 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detail_logreg(county, charge, hire_lawyer, public_defender, prior_hearings, plea):\n",
    "    try:\n",
    "        logreg = LogisticRegression()\n",
    "        df = df_full[(df_full['Court'] == county) & (df_full['ChargeType'] == charge)]\n",
    "        df_dummied = pd.get_dummies(df, columns = ['DefenseAttorney', 'Complainant', 'HearingPlea'], drop_first = True)\n",
    "        df_model = df_dummied[df_dummied['Contested'] == 1]\n",
    "        X = df_model.drop(columns = drop_features)\n",
    "        features = X.columns\n",
    "        y = df_model['Total_Positive']\n",
    "        baseline = y.value_counts(normalize = True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "        ss = StandardScaler()\n",
    "        X_train_sc = ss.fit_transform(X_train)\n",
    "        X_test_sc = ss.transform(X_test)\n",
    "        logreg.fit(X_train_sc, y_train)\n",
    "        cv_train = cross_val_score(logreg, X_train_sc, y_train)\n",
    "        cv_test = cross_val_score(logreg, X_test_sc, y_test)\n",
    "        \n",
    "        # sample prediction\n",
    "        X_real_test = pd.DataFrame\n",
    "        X_real_test = pd.DataFrame(index = [0], columns = [features])\n",
    "        X_real_test.fillna(0, inplace = True)\n",
    "        if hire_lawyer == 'Yes':\n",
    "            X_real_test.loc['HadLawyer'] = 1\n",
    "        if public_defender == 'Yes':\n",
    "            X_real_test.loc['PublicDefender'] = 1\n",
    "        X_real_test.loc['prior_hearings'] = prior_hearings\n",
    "        X_real_test.loc['TimeSinceOffense'] = df_model['TimeSinceOffense'].mean()\n",
    "        if plea == 'Guilty':\n",
    "            X_real_test.loc['HearingPlea_Guilty'] = 1\n",
    "        elif plea == 'Not Guilty':\n",
    "            X_real_test.loc['HearingPlea_Not Guilty'] = 1\n",
    "        elif plea == 'Nolo Contendere':\n",
    "            X_real_test.loc['HearingPlea_Nolo Contendere'] = 1\n",
    "        elif plea == 'Tried In Absentia':\n",
    "            X_real_test.loc['HearingPlea_Tried in Absentia'] = 1\n",
    "        elif plea == 'Unknown':\n",
    "            X_real_test.loc['HearingPlea_Unknown'] = 1\n",
    "        X_real_test_sc = ss.transform(X_real_test)\n",
    "        prediction = logreg.predict(X_real_test_sc)[0]\n",
    "        prediction_prob = logreg.predict_proba(X_real_test_sc)[0]\n",
    "\n",
    "        # coefficients\n",
    "        coefficients = logreg.coef_\n",
    "        coef_df = pd.DataFrame(coefficients, columns = features).T\n",
    "        coef_df['change_odds_ratio'] = coef_df.apply(lambda x: log_odds(x))\n",
    "        coef_df.rename(columns = {0: 'logreg_coefficient'}, inplace = True)\n",
    "        coef_df_top = coef_df.sort_values(by = 'logreg_coefficient', ascending = False).head()\n",
    "        coef_df_bottom = coef_df.sort_values(by = 'logreg_coefficient', ascending = False).tail()\n",
    "        coef_df_all = pd.concat([coef_df_top, coef_df_bottom])\n",
    "        \n",
    "        # real predictions\n",
    "        outcome_preds = logreg.predict(X_test_sc)\n",
    "        confusion_matrix(y_test, outcome_preds)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, outcome_preds).ravel()\n",
    "    \n",
    "        print(\"Model for {} in {}\".format(charge, county))\n",
    "        print(\"Baseline:\")\n",
    "        print(baseline)\n",
    "        print(\"-----\")\n",
    "        print(\"Predicting Your Outcome:\")\n",
    "        if prediction == 1:\n",
    "            print(\"You will be victorious!\")\n",
    "        elif prediction == 0:\n",
    "            print(\"You will fail...\")\n",
    "        print(\"Probability of success according to this model:\", prediction_prob[1])\n",
    "        print(\"-----\")\n",
    "        print(\"How Good is This Model?\")\n",
    "        print(\"Train Accuracy Scores:\", cv_train, \"Train Average Accuracy:\", cv_train.mean())\n",
    "        print(\"Test Accuracy Scores:\", cv_test, \"Test Average Accuracy:\", cv_test.mean())\n",
    "        print(\"-----\")\n",
    "        print(\"Classification Metrics:\")\n",
    "        print(\"True Negatives: %s\" % tn)\n",
    "        print(\"False Positives: %s\" % fp)\n",
    "        print(\"False Negatives: %s\" % fn)\n",
    "        print(\"True Positives: %s\" % tp)\n",
    "        print(\"-----\")\n",
    "        print(\"Accuracy: %s\" % ((tp + tn) / (tn + fp + fn + tp)))\n",
    "        print(\"Misclassification Rate: %s\" % ((fp + fn) / (tn + fp + fn + tp)))\n",
    "        print(\"-----\")\n",
    "        print(\"Sensitivity/Recall (True Positive Rate): %s\" % ((tp) / (tp + fn)))\n",
    "        print(\"Specificity (True Negative Rate): %s\" % ((tn) / (tn + fp)))\n",
    "        print(\"False Positive Rate: %s\" % ((fp) / (tp + fn)))\n",
    "        print(\"Precision: %s\" % ((tp) / (tp + fp)))\n",
    "        print(\"-----\")\n",
    "        print(\"Factors that Help the Case\")\n",
    "        print(coef_df_top)\n",
    "        print(\"-----\")\n",
    "        print(\"Factors that Hurt the Case\")\n",
    "        print(coef_df_bottom)\n",
    "    except:\n",
    "        print(\"Thinking...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693ed97b7761480ca5e47996cdf79855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='county', options=('Accomack County', 'Albemarle County', 'Alexandr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = interactive(detail_logreg, county = counties, charge = charges, hire_lawyer = had_lawyer_options, \n",
    "                public_defender = public_defender, prior_hearings = prior_hearings, plea = plea)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does Race Make a Difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_race1(county, charge, *args):\n",
    "    df_test = df_full[(df_full['ChargeType'] == charge) &\n",
    "                         (df_full['Court'] == county) &\n",
    "                         (df_full['Contested'] == 1)]\n",
    "    if len(args[0]) == 2:\n",
    "        test = stats.f_oneway(df_test[df_test['Race'] == str(args[0][0])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][1])]['Total_Positive'])\n",
    "    elif len(args[0]) == 3:\n",
    "        test = stats.f_oneway(df_test[df_test['Race'] == str(args[0][0])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][1])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][2])]['Total_Positive'])\n",
    "    elif len(args[0]) == 4:\n",
    "        test = stats.f_oneway(df_test[df_test['Race'] == str(args[0][0])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][1])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][2])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][3])]['Total_Positive'])\n",
    "    elif len(args[0]) == 5:\n",
    "        test = stats.f_oneway(df_test[df_test['Race'] == str(args[0][0])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][1])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][2])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][3])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][4])]['Total_Positive'])\n",
    "    elif len(args[0]) == 6:\n",
    "        test = stats.f_oneway(df_test[df_test['Race'] == str(args[0][0])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][1])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][2])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][3])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][4])]['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == str(args[0][5])]['Total_Positive'])\n",
    "\n",
    "    percent_race = df_test['Race'].value_counts(normalize = True)\n",
    "    p_value = test.pvalue\n",
    "   \n",
    "    print(\"Comparing mean outcomes for these populations contesting {} charges in {}:\".format(charge, county))\n",
    "    for i in args[0]:\n",
    "        print(\"    \", i)\n",
    "    print(\"P-value:\", p_value)\n",
    "    if p_value <= 0.01:\n",
    "        print(\"The p-value is sufficiently small that we can reject the null hypothesis and accept the alternative hypothesis: that there is a statistically significant difference in defense outcomes for these groups based on race.\")\n",
    "    if p_value > 0.01:\n",
    "        print(\"The p-value is not small enough to reject the null hypothesis. We cannot draw a conclusion about how outcomes differ by race for this charge.\")\n",
    "    print(\"--------------\")\n",
    "    print(\"Demographic makeup of defendees contesting {} charges in {}:\".format(charge, county))\n",
    "    print(percent_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_race(county, charge, comparison):\n",
    "    df_test = df_full[(df_full['ChargeType'] == charge) &\n",
    "                         (df_full['Court'] == county) &\n",
    "                         (df_full['Contested'] == 1)]\n",
    "    if comparison == 'White, Black':\n",
    "        test = stats.f_oneway(df_test[df_test['Race'] == 'White Caucasian(Non-Hispanic)']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'Black(Non-Hispanic)']['Total_Positive'])\n",
    "    elif comparison == 'White, Black, Latino':\n",
    "        test = stats.f_oneway(df_test[df_test['Race'] == 'White Caucasian(Non-Hispanic)']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'Black(Non-Hispanic)']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'Hispanic']['Total_Positive'])\n",
    "    elif comparison == 'White, Black, Latino, Asian Or Pacific Islander':\n",
    "        test = stats.f_oneway(df_test[df_test['Race'] == 'White Caucasian(Non-Hispanic)']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'Black(Non-Hispanic)']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'Hispanic']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'Asian Or Pacific Islander']['Total_Positive'])\n",
    "    elif comparison == 'White, Black, Latino, Asian Or Pacific Islander, Native American':\n",
    "        test = stats.f_oneway(df_test[df_test['Race'] == 'White Caucasian(Non-Hispanic)']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'Black(Non-Hispanic)']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'Hispanic']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'Asian Or Pacific Islander']['Total_Positive'],\n",
    "                          df_test[df_test['Race'] == 'American Indian']['Total_Positive'])\n",
    "\n",
    "    percent_race = df_test['Race'].value_counts(normalize = True)\n",
    "    p_value = test.pvalue\n",
    "   \n",
    "    print(\"Comparing mean outcomes for these populations contesting {} charges in {}:\".format(charge, county))\n",
    "    print(comparison)\n",
    "    print(\"-----\")\n",
    "    print(\"P-value:\", p_value)\n",
    "    if p_value <= 0.01:\n",
    "        print(\"YES. Race makes a difference.\")\n",
    "        print(\"The p-value is sufficiently small that we can reject the null hypothesis and accept the alternative hypothesis: \"\n",
    "              \"that there is a statistically significant difference in defense outcomes for these groups based on race.\")\n",
    "    if p_value > 0.01:\n",
    "        print(\"Inconclusive.\")\n",
    "        print(\"The p-value is not small enough to reject the null hypothesis. \"\n",
    "              \"We cannot draw a conclusion about how outcomes differ by race for this charge.\")\n",
    "    print(\"--------------\")\n",
    "    print(\"Demographic makeup of defendants contesting {} charges in {}:\".format(charge, county))\n",
    "    print(percent_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = ['White, Black', \n",
    "        'White, Black, Latino',\n",
    "        'White, Black, Latino, Asian Or Pacific Islander',\n",
    "        'White, Black, Latino, Asian Or Pacific Islander, Native American']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58df69cee86d459aaa15cd9772a123f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='county', options=('Accomack County', 'Albemarle County', 'Alexandr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anova_race = interactive(anova_race, county = counties, charge = charges, comparison = comps)\n",
    "display(anova_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does Type of Defense Matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_defense(county, charge, defense_comparison):\n",
    "    df_test = df_full[(df_full['ChargeType'] == charge) &\n",
    "                         (df_full['Court'] == county) &\n",
    "                         (df_full['Contested'] == 1)]\n",
    "    if defense_comparison == 'Include Defending Self':\n",
    "        test = stats.f_oneway(df_test[(df_test['HadLawyer'] == 1) &\n",
    "                                 (df_test['PublicDefender'] == 0)]['Total_Positive'],\n",
    "                   df_test[(df_test['HadLawyer'] == 1) &\n",
    "                          (df_test['PublicDefender'] == 1)]['Total_Positive'],\n",
    "                   df_test[df_test['HadLawyer'] == 0]['Total_Positive'])\n",
    "        print(\"Comparing mean outcomes for defendees with public defenders, private lawyers, and no lawyers contesting {} charges in {} based on defense strategy:\".format(charge, county))\n",
    "    \n",
    "    elif defense_comparison == 'Only Lawyers':\n",
    "        test = stats.f_oneway(df_test[(df_test['HadLawyer'] == 1) &\n",
    "                                 (df_test['PublicDefender'] == 0)]['Total_Positive'],\n",
    "                   df_test[(df_test['HadLawyer'] == 1) &\n",
    "                          (df_test['PublicDefender'] == 1)]['Total_Positive'])\n",
    "        print(\"Comparing mean outcomes for defendants \"\n",
    "              \"contesting {} charges in {} based on defense strategy:\".format(charge, county))\n",
    "    \n",
    "    percent_had_lawyer = df_test['HadLawyer'].value_counts(normalize = True)\n",
    "    percent_had_pd = df_test[df_test['HadLawyer'] == 1]['PublicDefender'].value_counts(normalize = True)\n",
    "    p_value = test.pvalue\n",
    "    \n",
    "    no_pd = df_test[(df_test['DefenseAttorney'] != 0) & (df_test['DefenseAttorney'] != 'publicdefender')]\n",
    "    top_5 = no_pd.groupby('DefenseAttorney')['Outcome_Positive'].sum().sort_values(ascending = False)[0:5]\n",
    "    \n",
    "    print(\"P-value:\", p_value)\n",
    "    if p_value <= 0.01:\n",
    "        print(\"YES, defense strategy matters.\")\n",
    "        print(\"The p-value is sufficiently small that we can reject the null hypothesis and \"\n",
    "        \"accept the alternative hypothesis: that there is a statistically significant difference \"\n",
    "        \"in defense outcomes for these groups based on defense strategy.\")\n",
    "    if p_value > 0.01:\n",
    "        print(\"Inconclusive.\")\n",
    "        print(\"The p-value is not small enough to reject the null hypothesis. We cannot draw \"\n",
    "        \"a conclusion about how outcomes differ by defense strategy for this charge.\")\n",
    "    print(\"--------------\")\n",
    "    print(\"Defendants who had lawyers to help contest {} charges in {}:\".format(charge, county))\n",
    "    print(percent_had_lawyer)\n",
    "    print(\"Of those with lawyers, the percentage with public defenders:\")\n",
    "    print(percent_had_pd)\n",
    "    print('-------')\n",
    "    print('Private Lawyers Who Won the Most (by # of cases)')\n",
    "    print(top_5)\n",
    "    print('-------')\n",
    "    print('Top Lawyers\\' Win Rates for This Charge')\n",
    "    for i in top_5.index:\n",
    "        print(i, no_pd[no_pd['DefenseAttorney'] == i]['Outcome_Positive'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "defenses = ['Include Defending Self', 'Only Lawyers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97503d6384284e8d8f339baf7c9fa23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='county', options=('Accomack County', 'Albemarle County', 'Alexandr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "defense = interactive(anova_defense, county = counties, charge = charges, \n",
    "                      defense_comparison = defenses)\n",
    "display(defense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
